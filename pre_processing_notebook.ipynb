{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HARPS / ESPRESSO Data Pre-processing for WOBBLE analysis\n",
    "\n",
    "## Outputs that the code has to create:\n",
    "\n",
    "1. HDF5 data file that can be read by Wobble for analysis\n",
    "2. Pipeline RVs from HARPS / ESPRESSO (plot & results)\n",
    "\n",
    "## Ideas / things that might be nice\n",
    "\n",
    "1. folded RVs for the TESS TOIs assocatiated with the planet with fit?\n",
    "2. download the data automatically - at the start?\n",
    "\n",
    "## Inputs into the function\n",
    "\n",
    "1. A folder containing the CCF data for the observations\n",
    "\n",
    "## Steps I do manually currently, and the scripts that are used to do them.\n",
    "\n",
    "1. Identify missing calibration files\n",
    "    - get_HARPS_calib.py - script by Meg B? defines a function that checks in the header for the name of the calibration file and creates a text file of the missing files.\n",
    "    - consider how this might need to change for ESPRESSO\n",
    "2. Download calibration files identified in 1.\n",
    "    - use a script by Andy Casey called 'prepare_download' that (I think) logs into the ESO site and gets the download of the files ready, without actually downloading anything yet.\n",
    "    - in a terminal I then change directory to the data directory and run 'cd data; sh download.sh'\n",
    "3. Extract all the .tar files\n",
    "4. Run from_HARPS / from_ESPRESSO (what do these functions actually do?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Main idea: function that takes in the data directory as an argument and does all of the above."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\"\n",
    "IMPORTING PACKAGES\n",
    "\"\"\"\n",
    "import wobble\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "from astropy.io import fits\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from astroquery.eso import Eso as ESO"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/z5345592/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def missing_wavelength_files(filelist):\n",
    "    \"\"\"identifies missing HARPS calibration files and returns them as a list\"\"\"\n",
    "    missing_files = []\n",
    "    for f in filelist:\n",
    "        path = f[0:str.rfind(f,'/')+1]\n",
    "        sp = fits.open(f)\n",
    "        header = sp[0].header\n",
    "        wave_file = header['HIERARCH ESO DRS CAL TH FILE']\n",
    "        if os.path.isfile(path+wave_file):\n",
    "            continue\n",
    "        else:\n",
    "            missing_files = np.append(missing_files, wave_file)\n",
    "\n",
    "    return np.unique(missing_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#is there a reason why this can't be rolled up with the previous function?\n",
    "\n",
    "if __name__ == '__main__': #\n",
    "    filelist = glob.glob('THE CCF FILES IN DATA DIRECTORY')\n",
    "    np.savetxt('missing_files.txt', missing_files, fmt='%s')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'missing_files' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b90d3eb1f8dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'THE CCF FILES IN DATA DIRECTORY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missing_files.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'missing_files' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this is 'prepare_download.py' in its entirety (except for package imports that have been rolled in above:\n",
    "# note - all other comments below are in the original code - not from Andrew Jolly\n",
    "\n",
    "def prepare_download(): #set a directory\n",
    "\n",
    "    cwd = '/home/z5345592/projects/harps_espresso_tess/' \n",
    "\n",
    "    ESO_USERNAME = \"andrewjolly\"\n",
    "\n",
    "    SKIP = 0 # Skip how many batches at the start? (for if you are re-running this..)\n",
    "    BATCH = 2000 # How many datasets should we get per ESO request?\n",
    "    WAIT_TIME = 60 # Seconds between asking ESO if they have prepared our request\n",
    "    DATA_DIR = os.path.join(cwd, \"data\")\n",
    "    MISSING_FILES_PATH = os.path.join(cwd, \"missing_files.txt\")\n",
    "\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "\n",
    "    with open(MISSING_FILES_PATH, \"r\") as fp:\n",
    "        missing_files = [line.strip() for line in fp.readlines()]\n",
    "\n",
    "    records = []\n",
    "    for missing_file in missing_files:\n",
    "        tar_file = \"SAF+{}.tar\".format(missing_file.split(\"_\")[0])\n",
    "        # Check to see if we have this filename already.\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, tar_file)):\n",
    "            records.append(tar_file)\n",
    "\n",
    "    N = len(records)\n",
    "    I = int(np.ceil(N/float(BATCH)))\n",
    "\n",
    "    remote_paths = []\n",
    "\n",
    "    print(\"In total we will request {} records in {} requests\".format(N, I))\n",
    "\n",
    "    for i in range(I):\n",
    "\n",
    "        if i < SKIP:\n",
    "            print(\"Skipping {}\".format(i + 1))\n",
    "            continue\n",
    "\n",
    "        print(\"Starting with batch number {}/{}\".format(i + 1, I))\n",
    "\n",
    "        data = [(\"dataset\", dataset) for dataset in records[i*BATCH:(i + 1)*BATCH]]\n",
    "\n",
    "        # Login to ESO.\n",
    "        eso = ESO()\n",
    "        eso.login(ESO_USERNAME)\n",
    "\n",
    "        prepare_response = eso._session.request(\"POST\",\n",
    "            \"http://dataportal.eso.org/rh/confirmation\", data=data)\n",
    "        assert prepare_response.ok\n",
    "\n",
    "        # Additional payload items required for confirmation.\n",
    "        data += [\n",
    "            (\"requestDescription\", \"\"),\n",
    "            (\"deliveryMediaType\", \"WEB\"), # OR USB_DISK --> Holy shit what the fuck!\n",
    "            (\"requestCommand\", \"SELECTIVE_HOTFLY\"),\n",
    "            (\"submit\", \"Submit\")\n",
    "        ]\n",
    "\n",
    "        confirmation_response = eso._session.request(\"POST\", \n",
    "            \"http://dataportal.eso.org/rh/requests/{}/submission\".format(ESO_USERNAME),\n",
    "            data=data)\n",
    "        assert confirmation_response.ok\n",
    "\n",
    "        # Parse the request number so that we can get a download script from ESO later\n",
    "        _ = re.findall(\"Request #[0-9]+\\w\", confirmation_response.text)[0].split()[-1]\n",
    "        request_number = int(_.lstrip(\"#\"))\n",
    "\n",
    "        print(\"Retrieving remote paths for request number {}/{}: {}\".format(\n",
    "            i + 1, I, request_number))\n",
    "\n",
    "        # Check if ESO is ready for us.\n",
    "        while True:\n",
    "\n",
    "            url = \"https://dataportal.eso.org/rh/requests/{}\".format(ESO_USERNAME)\n",
    "            check_state = eso._request(\"GET\", url, cache=False)\n",
    "            root = BeautifulSoup(check_state.text, \"html5lib\")\n",
    "\n",
    "            link = root.find(href=\"/rh/requests/{}/{}\".format(\n",
    "                ESO_USERNAME, request_number))\n",
    "\n",
    "            image = link.find_next(\"img\")\n",
    "            state = image.attrs[\"alt\"]\n",
    "\n",
    "            print(\"Current state {} on request {} ({}/{})\".format(\n",
    "                state, request_number, i + 1, I))\n",
    "\n",
    "            if state != \"COMPLETE\":\n",
    "\n",
    "                # Remove anything from the astroquery cache.\n",
    "                for cached_file in glob(os.path.join(eso.cache_location, \"*\")):\n",
    "                    os.remove(cached_file)\n",
    "\n",
    "                print(\"Sleeping for {} seconds..\".format(WAIT_TIME))\n",
    "                time.sleep(WAIT_TIME)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        response = eso._request(\"GET\", \"{}/{}/script\".format(url, request_number))\n",
    "        \n",
    "        paths = response.text.split(\"__EOF__\")[-2].split(\"\\n\")[1:-2]\n",
    "        print(\"Found {} remote paths for request_number {}\".format(\n",
    "            len(paths), request_number))\n",
    "        remote_paths.extend(paths)\n",
    "\n",
    "        # Remove anything from the astroquery cache.\n",
    "        for cached_file in glob(os.path.join(eso.cache_location, \"*\")):\n",
    "            os.remove(cached_file)\n",
    "        \n",
    "        # We have all the remote paths for this request. At ESO's advice, let's\n",
    "        # wait another 60 seconds before starting our new request.\n",
    "        if I > i + 1:\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Prepare the script for downloading.\n",
    "    template_path = os.path.join(cwd, \"download.sh.template\")\n",
    "    with open(template_path, \"r\") as fp:\n",
    "        contents = fp.read()\n",
    "\n",
    "    script_path = os.path.join(DATA_DIR, \"download.sh\")\n",
    "    with open(script_path, \"w\") as fp:\n",
    "        fp.write(contents.replace(\"$$REMOTE_PATHS$$\", \"\\n\".join(remote_paths))\\\n",
    "                        .replace(\"$$ESO_USERNAME$$\", ESO_USERNAME))\n",
    "\n",
    "    print(\"Created script {0}\".format(script_path))\n",
    "    print(\"Now run `cd {}; sh {}` and enter your ESO password when requested.\"\\\n",
    "        .format(DATA_DIR, os.path.basename(script_path)))\n",
    "\n",
    "    return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#need a step here that runs the following command:\n",
    "\n",
    "#'cd data; sh download.sh'\n",
    "\n",
    "#question - could I just do this using the last line of the above?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## At this point I have the data and the calibration files in a data directory that has been defined.\n",
    "\n",
    "# Now need to unzip the TARS \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tar_unzipper(data_directory):\n",
    "    \"\"\"extracts all tar files in a directory into the same directory\"\"\"\n",
    "    for file in glob.glob(data_directory + '/*.tar'):\n",
    "        tar = tarfile.open(file, 'r')\n",
    "        tar.extractall(path = data_directory)\n",
    "        print('Extracting ' + file + ' to ' + data_directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#need a check here to see if the data is from HARPS or ESPRESSO here.\n",
    "\n",
    "data = wobble.Data() #create a wobble data object so it can be appended with the information from the CCF files\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    try:\n",
    "        sp = wobble.Spectrum()\n",
    "        sp.from_HARPS(filename, process = True)\n",
    "        data.append(sp)\n",
    "    except Exception as e:\n",
    "        print(\"File {0} failed; error: {1}\".format(filename, e))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.write(data_dir + star_name + instrument_name + 'pre_proc.hdf5') #writing the HDF5 file that contains all the info that wobble uses as well as the pipeline RV / dates etc that can create quick plots now"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\"\"\"Function to get name of star and name of instrument from files - make sure its same star, same instrument?\"\"\"\n",
    "\n",
    "def get_instrument_name(data_directory):\n",
    "    filelist = glob.glob(data_directory + '*ccf_G2_A.fits')\n",
    "    file = fits.open(filelist[0])\n",
    "    header = file[0].header\n",
    "    instrument_name = (header['INSTRUME'])\n",
    "    \n",
    "    return instrument_name  \n",
    "\n",
    "data_dir = '/home/z5345592/projects/eso_pre_processing/test_data/'\n",
    "print(get_instrument_name(data_dir))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HARPS\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_target_name(data_directory):\n",
    "    filelist = glob.glob(data_directory + '*ccf_G2_A.fits')\n",
    "    file = fits.open(filelist[0])\n",
    "    header = file[0].header\n",
    "    target_name = (header['OBJECT'])\n",
    "    \n",
    "    return target_name  \n",
    "\n",
    "data_dir = '/home/z5345592/projects/eso_pre_processing/test_data/'\n",
    "print(get_target_name(data_dir))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HD48611\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('py37': conda)"
  },
  "interpreter": {
   "hash": "e42cc0348417aa91ddc6a1cdb7ea3527b7ca2b5c78f15839e9082e9cb67a5386"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}